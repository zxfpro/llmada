"""
模型适配器
"""
from .client import OpenAIClient
from openai import OpenAI
import os
from volcenginesdkarkruntime import Ark
from llama_index.core.prompts import RichPromptTemplate
from llama_index.llms.openai.utils import to_openai_message_dicts
import requests


class ModelAdapter:
    def __init__(self):
        self.api_key = None
        self.model_name = None
        self.temperature = 0.7
        self.client = None

    def set_model(self, model_name: str):
        self.model_name = model_name
    
    def set_temperature(self, temperature:float):
        self.temperature = temperature

    def get_model(self)->list[str]:
        return ["详见 看见"]

    def product(self, prompt: str) -> str:
        raise NotImplementedError("This method should be implemented by subclasses")

    def chat(self, messages: list) -> str:
        raise NotImplementedError("This method should be implemented by subclasses")


class BianXieAdapter(ModelAdapter):
    """BianXie格式的适配器
    """
    def __init__(self, api_key: str = None, api_base: str = "https://api.bianxie.ai/v1/chat/completions"):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()
        self.client = OpenAIClient(api_key=api_key or os.getenv('BIANXIE_API_KEY') , api_base=api_base)
        self.model_name = "gemini-2.5-flash-preview-04-17-nothinking"

    def get_model(self):
        return ["详见 看见"]
    
    def get_modal_model(self):
        return ['gpt-4o']
    
    def product_modal(self, prompt: RichPromptTemplate) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        assert self.model_name in self.get_modal_model()
        messages = to_openai_message_dicts(prompt)
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.request(data).get('choices')[0].get('message').get('content')


    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.request(data).get('choices')[0].get('message').get('content')

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.request(data).get('choices')[0].get('message').get('content')
    
    def product_stream(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature,
            "stream": True
        }
        result_stream = self.client.request_stream(data)
        
        for i in result_stream:
            yield i

    def chat_stream(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """

        data = {
            "model": self.model_name,
            "messages": messages,
            'temperature': self.temperature,
            "stream": True
        }
        result_stream = self.client.request_stream(data)
        
        for i in result_stream:
            yield i




class KimiAdapter(ModelAdapter):
    """Kimi格式的适配器

    """
    def __init__(self, api_key: str = None, api_base: str = "https://api.moonshot.cn/v1",):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()
        
        self.client = OpenAI(api_key=api_key or os.getenv('MOONSHOT_API_KEY') , base_url=api_base)
        self.model_name = "moonshot-v1-128k"

    def get_model(self):
        return ["moonshot-v1-128k","moonshot-v1-128k","moonshot-v1-128k"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """

        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature,
            'stream': True #
        }
        return self.client.chat.completions.create(**data)
        # return self.client.chat.completions.create(**data).choices[0].message.content
    


class ArkAdapter(ModelAdapter):
    def __init__(self, api_key: str = None, api_base: str = None,):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()

        self.client = Ark(api_key=api_key or os.getenv('ARK_API_KEY'))
        self.model_name = "doubao-1-5-pro-256k-250115"

    def get_model(self):
        return ["doubao-1-5-pro-256k-250115"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content


def set_llama_index(api_key: str = None, api_base: str = "https://api.bianxieai.com/v1",model:str="gpt-4o",temperature: float =0.1):
    """初始化

    Args:
        api_key (str): API key for authentication.
        api_base (str): Base URL for the API endpoint.
    """

    from llama_index.llms.openai import OpenAI
    from llama_index.core import Settings
    from llama_index.embeddings.openai import OpenAIEmbedding

    api_key=api_key or os.getenv('BIANXIE_API_KEY')

    client = OpenAI(
        model=model,
        api_base=api_base,
        api_key=api_key,
        temperature=temperature,
    )
    Settings.llm = client

    embedding = OpenAIEmbedding(api_base=api_base,api_key=api_key)
    Settings.embed_model = embedding


