"""
模型适配器
"""
from .client import OpenAIClient
from openai import OpenAI
import os

class ModelAdapter:
    def __init__(self):
        self.api_key = None
        self.model_name = None
        self.temperature = 0.7
        self.client = None

    def set_model(self, model_name: str):
        self.model_name = model_name
    
    def set_temperature(self, temperature:float):
        self.temperature = temperature

    def get_model(self)->list[str]:
        return ["详见 看见"]

    def product(self, prompt: str) -> str:
        raise NotImplementedError("This method should be implemented by subclasses")

    def chat(self, messages: list) -> str:
        raise NotImplementedError("This method should be implemented by subclasses")


class BianXieAdapter(ModelAdapter):
    """BianXie格式的适配器
    """
    def __init__(self, api_key: str = None, api_base: str = "https://api.bianxie.ai/v1/chat/completions"):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()
        self.client = OpenAIClient(api_key=api_key or os.getenv('BIANXIE_API_KEY') , api_base=api_base)
        self.model_name = "gemini-2.5-flash-preview-04-17-nothinking"

    def get_model(self):
        return ["详见 看见"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.request(data).get('choices')[0].get('message').get('content')

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.request(data).get('choices')[0].get('message').get('content')
    
    def product_stream(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature,
            "stream": True
        }
        result_stream = self.client.request_stream(data)
        
        for i in result_stream:
            yield i

    def chat_stream(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """

        data = {
            "model": self.model_name,
            "messages": messages,
            'temperature': self.temperature,
            "stream": True
        }
        result_stream = self.client.request_stream(data)
        
        for i in result_stream:
            yield i

from openai import OpenAI

class KimiAdapter(ModelAdapter):
    """Kimi格式的适配器

    """
    def __init__(self, api_key: str = None, api_base: str = "https://api.moonshot.cn/v1",):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()
        
        self.client = OpenAI(api_key=api_key or os.getenv('MOONSHOT_API_KEY') , base_url=api_base)
        self.model_name = "moonshot-v1-128k"

    def get_model(self):
        return ["moonshot-v1-128k","moonshot-v1-128k","moonshot-v1-128k"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """

        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature,
            'stream': True #
        }
        return self.client.chat.completions.create(**data)
        # return self.client.chat.completions.create(**data).choices[0].message.content
    
from volcenginesdkarkruntime import Ark

class ArkAdapter(ModelAdapter):
    def __init__(self, api_key: str = None, api_base: str = None,):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()

        self.client = Ark(api_key=api_key or os.getenv('ARK_API_KEY'))
        self.model_name = "doubao-1-5-pro-256k-250115"

    def get_model(self):
        return ["doubao-1-5-pro-256k-250115"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

class OpenRouterAdapter(ModelAdapter):
    """OpenRouter格式的适配器
    不好用
    """
    def __init__(self, api_key: str = None, api_base: str = "https://openrouter.ai/api/v1",):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()
        
        self.client = OpenAI(api_key=api_key or os.getenv('OPENROUTER_API_KEY') , base_url=api_base)
        self.model_name = ""

    def get_model(self):
        return [""]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """
        data = {
            'model': self.model_name,
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        data = {
            'model': self.model_name,
            'messages': messages,
            'temperature': self.temperature
        }
        return self.client.chat.completions.create(**data).choices[0].message.content


# from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.prompts import RichPromptTemplate
from llama_index.llms.openai.utils import to_openai_message_dicts
import requests







class LLamaIndexAdapter(ModelAdapter):
    def __init__(self, api_key: str = None, api_base: str = "https://api.bianxieai.com/v1",model:str="gpt-4o",temperature: float =0.1):
        """初始化

        Args:
            api_key (str): API key for authentication.
            api_base (str): Base URL for the API endpoint.
        """
        super().__init__()


        api_key=api_key or os.getenv('BIANXIE_API_KEY')

        self.client = OpenAI(
            model=model,
            api_base=api_base,
            api_key=api_key,
            temperature=temperature,
            # max_tokens=None,
        )
        Settings.llm = self.client

        self.embedding = OpenAIEmbedding(api_base=api_base,api_key=api_key)
        Settings.embed_model = self.embedding


    def get_model(self):
        return ["gpt-4o"]

    def product(self, prompt: str) -> str:
        """Generate a response from the model based on a single prompt.

        Args:
            prompt (str): The input text prompt to generate a response for.

        Returns:
            str: The response generated by the model.
        """

        return self.client.complete(prompt=prompt)

        # return self.client.chat.completions.create(**data).choices[0].message.content

    def products(self):

        url = 'https://api.bianxie.ai/v1/chat/completions'
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer sk-tQ17YaQSAvb6REf474A112Eb57064c5d9f6a9599F96a35A6'  # 将这里换成你在便携AI聚合API后台生成的令牌
        }
        data = {
            'model': 'gpt-4o',
            'messages': to_openai_message_dicts(messages)
        }
        response = requests.post(url, headers=headers, json=data)
        print(response.json())

    def chat(self, messages: list) -> str:
        """Engage in a conversation with the model using a list of messages.

        Args:
            messages (list): A list of message dictionaries, each containing a role and content.

        Returns:
            str: The response generated by the model for the conversation.
        """
        pass


# prompt = RichPromptTemplate(
#     """
# Describe the following image:
# {{ image_path | image}}
# 工作
# """
# )
# messages = prompt.format_messages(image_path="./image.png")