{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LLMADA","text":""},{"location":"client/","title":"client","text":"<p>This path the project documentation </p> <p>client.py \u7528\u4e8e\u7ba1\u7406client</p>"},{"location":"client/#llmada.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>\u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd</p> Source code in <code>llmada/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"\n    \u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n        \"\"\"\n        \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n        \"\"\"\n        self.api_key = api_key\n        self.api_base = api_base\n        self.headers = {\n            'Content-Type': 'application/json',\n            'Authorization': f'Bearer {api_key}'\n        }\n\n    def request(self, params: dict) -&gt; dict:\n        \"\"\"\n        \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n        \"\"\"\n\n        try:\n            response = requests.post(self.api_base, headers=self.headers, json=params)\n            return response.json()\n        except Exception as e:\n            raise Exception(f\"API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient.__init__","title":"<code>__init__(api_key, api_base='https://api.bianxie.ai/v1/chat/completions')</code>","text":"<p>\u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef</p> Source code in <code>llmada/client.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n    \"\"\"\n    \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n    \"\"\"\n    self.api_key = api_key\n    self.api_base = api_base\n    self.headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {api_key}'\n    }\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient.request","title":"<code>request(params)</code>","text":"<p>\u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94</p> Source code in <code>llmada/client.py</code> <pre><code>def request(self, params: dict) -&gt; dict:\n    \"\"\"\n    \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n    \"\"\"\n\n    try:\n        response = requests.post(self.api_base, headers=self.headers, json=params)\n        return response.json()\n    except Exception as e:\n        raise Exception(f\"API request failed: {e}\")\n</code></pre>"},{"location":"core/","title":"core","text":"<p>This path the project documentation </p> <p>\u6a21\u578b\u9002\u914d\u5668</p>"},{"location":"core/#llmada.core.ArkAdapter","title":"<code>ArkAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> Source code in <code>llmada/core.py</code> <pre><code>class ArkAdapter(ModelAdapter):\n    def __init__(self, api_key: str, api_base: str = None,):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n\n        from volcenginesdkarkruntime import Ark\n\n        self.client = Ark(api_key=api_key)\n\n    def get_model(self):\n        return [\"doubao-1-5-pro-256k-250115\"]\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"core/#llmada.core.ArkAdapter.__init__","title":"<code>__init__(api_key, api_base=None)</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> required <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>None</code> Source code in <code>llmada/core.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = None,):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n\n    from volcenginesdkarkruntime import Ark\n\n    self.client = Ark(api_key=api_key)\n</code></pre>"},{"location":"core/#llmada.core.ArkAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"core/#llmada.core.ArkAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"core/#llmada.core.BianXieAdapter","title":"<code>BianXieAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> <p>BianXie\u683c\u5f0f\u7684\u9002\u914d\u5668</p> Source code in <code>llmada/core.py</code> <pre><code>class BianXieAdapter(ModelAdapter):\n    \"\"\"BianXie\u683c\u5f0f\u7684\u9002\u914d\u5668\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n        self.client = OpenAIClient(api_key=api_key, api_base=api_base)\n\n    def get_model(self):\n        return [\"\u8be6\u89c1 \u770b\u89c1\"]\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        return self.client.request(data).get('choices')[0].get('message').get('content')\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        return self.client.request(data).get('choices')[0].get('message').get('content')\n</code></pre>"},{"location":"core/#llmada.core.BianXieAdapter.__init__","title":"<code>__init__(api_key, api_base='https://api.bianxie.ai/v1/chat/completions')</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> required <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>'https://api.bianxie.ai/v1/chat/completions'</code> Source code in <code>llmada/core.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n    self.client = OpenAIClient(api_key=api_key, api_base=api_base)\n</code></pre>"},{"location":"core/#llmada.core.BianXieAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    return self.client.request(data).get('choices')[0].get('message').get('content')\n</code></pre>"},{"location":"core/#llmada.core.BianXieAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    return self.client.request(data).get('choices')[0].get('message').get('content')\n</code></pre>"},{"location":"core/#llmada.core.KimiAdapter","title":"<code>KimiAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> <p>Kimi\u683c\u5f0f\u7684\u9002\u914d\u5668</p> Source code in <code>llmada/core.py</code> <pre><code>class KimiAdapter(ModelAdapter):\n    \"\"\"Kimi\u683c\u5f0f\u7684\u9002\u914d\u5668\n\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.moonshot.cn/v1\",):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n\n        self.client = OpenAI(api_key=api_key, base_url=api_base)\n\n    def get_model(self):\n        return [\"moonshot-v1-128k\",\"moonshot-v1-128k\",\"moonshot-v1-128k\"]\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"core/#llmada.core.KimiAdapter.__init__","title":"<code>__init__(api_key, api_base='https://api.moonshot.cn/v1')</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> required <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>'https://api.moonshot.cn/v1'</code> Source code in <code>llmada/core.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.moonshot.cn/v1\",):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n\n    self.client = OpenAI(api_key=api_key, base_url=api_base)\n</code></pre>"},{"location":"core/#llmada.core.KimiAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"core/#llmada.core.KimiAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"use_case/","title":"\u4f7f\u7528\u6848\u4f8b","text":""},{"location":"use_case/#product","title":"\u4fbf\u643a\u6a21\u578b\u4f7f\u7528product","text":"<pre><code>from llmada import BianXieAdapter\n\nbianxie = BianXieAdapter(api_key=os.getenv('BIANXIE'))\n\nbianxie.set_model('gpt-4o')\n\nbianxie.product(prompt='\u4f60\u597d')\n</code></pre>"},{"location":"use_case/#chat","title":"\u4fbf\u643a\u6a21\u578b\u4f7f\u7528chat","text":"<pre><code>\nfrom llmada import BianXieAdapter\n\nbianxie = BianXieAdapter(api_key=os.getenv('BIANXIE'))\n\nbianxie.set_model('gpt-4o')\n\nbianxie.chat(messages=[{'role': 'user', 'content': 'hello'}],)\n</code></pre>"},{"location":"use_case/#kimichat","title":"kimi\u6a21\u578b\u4f7f\u7528chat","text":"<pre><code>from llmada import KimiAdapter\nkimi = KimiAdapter(api_key=os.getenv('MOONSHOT_API_KEY'))\nkimi.get_model()\nkimi.set_model('moonshot-v1-128k')\nkimi.chat(messages=[{'role': 'user', 'content': 'hello'}],)\n</code></pre>"},{"location":"use_case/#kimi-product","title":"kimi \u6a21\u578b\u4f7f\u7528product","text":"<pre><code>from llmada import KimiAdapter\nkimi = KimiAdapter(api_key=os.getenv('MOONSHOT_API_KEY'))\nkimi.get_model()\nkimi.set_model('moonshot-v1-128k')\nx = kimi.product(prompt='hello')\n</code></pre>"}]}