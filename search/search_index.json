{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LLMADA","text":""},{"location":"client/","title":"client","text":"<p>This path the project documentation </p> <p>client.py \u7528\u4e8e\u7ba1\u7406client</p>"},{"location":"client/#llmada.client.LlamaIndexOpenAIClient","title":"<code>LlamaIndexOpenAIClient</code>","text":"<p>\u4f7f\u7528 llama-index \u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd</p> Source code in <code>llmada/client.py</code> <pre><code>class LlamaIndexOpenAIClient:\n    \"\"\"\n    \u4f7f\u7528 llama-index \u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.openai.com/v1\"):\n        \"\"\"\n        \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n        \"\"\"\n        self.api_key = api_key\n        self.api_base = api_base\n\n        # \u521d\u59cb\u5316 LLM \u548c Embedding \u6a21\u578b\n        self.llm = OpenAI(\n            model=\"gpt-3.5-turbo\",\n            api_base=self.api_base,\n            api_key=self.api_key,\n            temperature=0.1\n        )\n        self.embed_model = OpenAIEmbedding(\n            api_base=self.api_base,\n            api_key=self.api_key\n        )\n        Settings.embed_model = self.embed_model\n        Settings.llm = self.llm\n\n    def predict(self, messages: Iterator[ChatMessage], **kwargs: Any) -&gt; ChatMessage:\n        \"\"\"\n        \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n        \"\"\"\n        try:\n            response = self.llm.predict(messages, **kwargs)\n            return response\n        except Exception as e:\n            raise Exception(f\"API request failed: {e}\")\n\n    def stream_predict(self, messages: Iterator[ChatMessage], **kwargs: Any) -&gt; Iterator[ChatMessage]:\n        \"\"\"\n        \u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668\n        \"\"\"\n        try:\n            response = self.llm.stream_predict(messages, **kwargs)\n            return response\n        except Exception as e:\n            raise Exception(f\"Stream API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.LlamaIndexOpenAIClient.__init__","title":"<code>__init__(api_key, api_base='https://api.openai.com/v1')</code>","text":"<p>\u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef</p> Source code in <code>llmada/client.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.openai.com/v1\"):\n    \"\"\"\n    \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n    \"\"\"\n    self.api_key = api_key\n    self.api_base = api_base\n\n    # \u521d\u59cb\u5316 LLM \u548c Embedding \u6a21\u578b\n    self.llm = OpenAI(\n        model=\"gpt-3.5-turbo\",\n        api_base=self.api_base,\n        api_key=self.api_key,\n        temperature=0.1\n    )\n    self.embed_model = OpenAIEmbedding(\n        api_base=self.api_base,\n        api_key=self.api_key\n    )\n    Settings.embed_model = self.embed_model\n    Settings.llm = self.llm\n</code></pre>"},{"location":"client/#llmada.client.LlamaIndexOpenAIClient.predict","title":"<code>predict(messages, **kwargs)</code>","text":"<p>\u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94</p> Source code in <code>llmada/client.py</code> <pre><code>def predict(self, messages: Iterator[ChatMessage], **kwargs: Any) -&gt; ChatMessage:\n    \"\"\"\n    \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n    \"\"\"\n    try:\n        response = self.llm.predict(messages, **kwargs)\n        return response\n    except Exception as e:\n        raise Exception(f\"API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.LlamaIndexOpenAIClient.stream_predict","title":"<code>stream_predict(messages, **kwargs)</code>","text":"<p>\u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668</p> Source code in <code>llmada/client.py</code> <pre><code>def stream_predict(self, messages: Iterator[ChatMessage], **kwargs: Any) -&gt; Iterator[ChatMessage]:\n    \"\"\"\n    \u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668\n    \"\"\"\n    try:\n        response = self.llm.stream_predict(messages, **kwargs)\n        return response\n    except Exception as e:\n        raise Exception(f\"Stream API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>\u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd</p> Source code in <code>llmada/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"\n    \u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.openai.com/v1\"):\n        \"\"\"\n        \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n        \"\"\"\n        openai.api_key = api_key\n        openai.api_base = api_base\n\n    def request(self, params: dict) -&gt; dict:\n        \"\"\"\n        \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n        \"\"\"\n        try:\n            response = openai.ChatCompletion.create(**params)\n            return response\n        except Exception as e:\n            raise Exception(f\"API request failed: {e}\")\n\n    def request_stream(self, params: dict) -&gt; iter:\n        \"\"\"\n        \u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668\n        \"\"\"\n        try:\n            response = openai.ChatCompletion.create(**params, stream=True)\n            for chunk in response:\n                yield chunk\n        except Exception as e:\n            raise Exception(f\"Stream API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient.__init__","title":"<code>__init__(api_key, api_base='https://api.openai.com/v1')</code>","text":"<p>\u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef</p> Source code in <code>llmada/client.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.openai.com/v1\"):\n    \"\"\"\n    \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n    \"\"\"\n    openai.api_key = api_key\n    openai.api_base = api_base\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient.request","title":"<code>request(params)</code>","text":"<p>\u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94</p> Source code in <code>llmada/client.py</code> <pre><code>def request(self, params: dict) -&gt; dict:\n    \"\"\"\n    \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n    \"\"\"\n    try:\n        response = openai.ChatCompletion.create(**params)\n        return response\n    except Exception as e:\n        raise Exception(f\"API request failed: {e}\")\n</code></pre>"},{"location":"client/#llmada.client.OpenAIClient.request_stream","title":"<code>request_stream(params)</code>","text":"<p>\u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668</p> Source code in <code>llmada/client.py</code> <pre><code>def request_stream(self, params: dict) -&gt; iter:\n    \"\"\"\n    \u6d41\u5f0f\u5bf9\u8bdd\uff1a\u9010\u5757\u63a5\u6536\u54cd\u5e94\u5e76\u8fd4\u56de\u751f\u6210\u5668\n    \"\"\"\n    try:\n        response = openai.ChatCompletion.create(**params, stream=True)\n        for chunk in response:\n            yield chunk\n    except Exception as e:\n        raise Exception(f\"Stream API request failed: {e}\")\n</code></pre>"},{"location":"core/","title":"Core","text":"<p>This path the project documentation </p> <p>\u6a21\u578b\u9002\u914d\u5668</p>"}]}