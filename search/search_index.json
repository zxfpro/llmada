{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LLMADA","text":""},{"location":"api_document/","title":"client","text":"<p>This path the project documentation </p>"},{"location":"api_document/#client","title":"client","text":"<p>client.py \u7528\u4e8e\u7ba1\u7406client</p>"},{"location":"api_document/#src.llmada.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>\u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd</p> Source code in <code>src/llmada/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"\n    \u4f7f\u7528 openai \u5b98\u65b9\u5305\u5bf9\u63a5 OpenAI API\uff0c\u652f\u6301\u7b80\u5355\u5bf9\u8bdd\u548c\u6d41\u5f0f\u5bf9\u8bdd\n    \"\"\"\n    def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n        \"\"\"\n        \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n        \"\"\"\n        self.api_key = api_key\n        self.api_base = api_base\n        self.headers = {\n            'Content-Type': 'application/json',\n            'Authorization': f'Bearer {api_key}'\n        }\n\n    def request(self, params: dict) -&gt; dict:\n        \"\"\"\n        \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n        \"\"\"\n\n        try:\n            time1 = time.time()\n            response = requests.post(self.api_base, headers=self.headers, json=params)\n            time2 = time.time()\n            print(time2-time1)\n            return response.json()\n        except Exception as e:\n            raise Exception(f\"API request failed: {e}\")\n\n\n    def request_stream(self, params: dict) -&gt; dict:\n        \"\"\"\n        \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u6d41\u5f0f\u54cd\u5e94\n        \"\"\"\n        try:\n            time1 = time.time()\n            response = requests.post(self.api_base, headers=self.headers, json=params, stream=True)\n\n            # \u68c0\u67e5HTTP\u72b6\u6001\u7801\n            if response.status_code == 200:\n                print(\"Received streaming response:\")\n                # \u9010\u884c\u5904\u7406\u54cd\u5e94\u5185\u5bb9\n                # iter_lines() \u8fed\u4ee3\u54cd\u5e94\u5185\u5bb9\uff0c\u6309\u884c\u5206\u5272\uff0c\u5e76\u89e3\u7801\n                time2 = time.time()\n                for line in response.iter_lines():\n                    # lines are bytes, convert to string\n                    line = line.decode('utf-8')\n\n                    # Server-Sent Events (SSE) messages start with 'data: '\n                    # and the stream ends with 'data: [DONE]'\n                    if line.startswith('data:'):\n                        # Extract the JSON string after 'data: '\n                        json_str = line[len('data:'):].strip()\n\n                        if json_str == '[DONE]':\n                            break # End of stream\n\n                        if json_str: # Ensure it's not an empty data line\n                            try:\n                                # Parse the JSON string into a dictionary\n                                chunk = json.loads(json_str)\n\n                                # Extract the content chunk (similar structure to OpenAI API)\n                                # Check if choices and delta exist before accessing content\n                                if chunk and 'choices' in chunk and len(chunk['choices']) &gt; 0:\n                                    delta = chunk['choices'][0].get('delta')\n                                    if delta and 'content' in delta:\n                                        content_chunk = delta['content']\n                                        # Print the chunk without a newline, immediately flushing the output\n                                        # print(content_chunk, end='', flush=True)\n                                        yield content_chunk\n\n                            except json.JSONDecodeError as e:\n                                print(f\"\\nError decoding JSON chunk: {e}\\nChunk: {json_str}\")\n                            except Exception as e:\n                                print(f\"\\nError processing chunk: {e}\\nChunk data: {chunk}\")\n\n\n                print(f\"\\n(Streaming finished) {time2-time1}\") # Add a newline after the stream is complete\n\n            else:\n                # Handle non-200 responses\n                print(f\"Error: Received status code {response.status_code}\")\n                print(\"Response body:\")\n                print(response.text) # Print the full error response if not streaming\n\n        except requests.exceptions.RequestException as e:\n            # TODO if \"439\" in ccc\n            print(f\"Request Error: {e}\")\n        except Exception as e:\n            print(f\"An unexpected error occurred: {e}\")\n\n    def request_modal(self):\n\n        def get_img_content(inputs = str):\n            if is_url:\n                return \"https://github.com/dianping/cat/raw/master/cat-home/src/main/webapp/images/logo/cat_logo03.png\"\n            else:\n                def image_to_base64(image_path):\n                    with open(image_path, \"rb\") as image_file:\n                        image_data = image_file.read()\n                        base64_encoded_data = base64.b64encode(image_data)\n                        base64_encoded_str = base64_encoded_data.decode('utf-8')\n                        return base64_encoded_str    \n                image_base64 = image_to_base64(inputs)\n                return f\"data:image/jpeg;base64,{image_base64}\"\n\n\n        data = {\n            'model': 'gpt-4-vision-preview',\n            'messages': [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": \"\u8fd9\u5f20\u56fe\u7247\u7684\u56fe\u6807\u662f\u4e2a\u4ec0\u4e48\u52a8\u7269\uff1f\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": get_img_content('111.png')\n                            }\n                        }\n                    ]\n                }\n            ],\n        }\n\n        response = requests.post(self.api_base, headers=self.headers, json=data)\n        return response.json()\n</code></pre>"},{"location":"api_document/#src.llmada.client.OpenAIClient.__init__","title":"<code>__init__(api_key, api_base='https://api.bianxie.ai/v1/chat/completions')</code>","text":"<p>\u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef</p> Source code in <code>src/llmada/client.py</code> <pre><code>def __init__(self, api_key: str, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n    \"\"\"\n    \u521d\u59cb\u5316 OpenAI \u5ba2\u6237\u7aef\n    \"\"\"\n    self.api_key = api_key\n    self.api_base = api_base\n    self.headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {api_key}'\n    }\n</code></pre>"},{"location":"api_document/#src.llmada.client.OpenAIClient.request","title":"<code>request(params)</code>","text":"<p>\u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94</p> Source code in <code>src/llmada/client.py</code> <pre><code>def request(self, params: dict) -&gt; dict:\n    \"\"\"\n    \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u5b8c\u6574\u54cd\u5e94\n    \"\"\"\n\n    try:\n        time1 = time.time()\n        response = requests.post(self.api_base, headers=self.headers, json=params)\n        time2 = time.time()\n        print(time2-time1)\n        return response.json()\n    except Exception as e:\n        raise Exception(f\"API request failed: {e}\")\n</code></pre>"},{"location":"api_document/#src.llmada.client.OpenAIClient.request_stream","title":"<code>request_stream(params)</code>","text":"<p>\u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u6d41\u5f0f\u54cd\u5e94</p> Source code in <code>src/llmada/client.py</code> <pre><code>def request_stream(self, params: dict) -&gt; dict:\n    \"\"\"\n    \u7b80\u5355\u5bf9\u8bdd\uff1a\u76f4\u63a5\u8c03\u7528 OpenAI API \u5e76\u8fd4\u56de\u6d41\u5f0f\u54cd\u5e94\n    \"\"\"\n    try:\n        time1 = time.time()\n        response = requests.post(self.api_base, headers=self.headers, json=params, stream=True)\n\n        # \u68c0\u67e5HTTP\u72b6\u6001\u7801\n        if response.status_code == 200:\n            print(\"Received streaming response:\")\n            # \u9010\u884c\u5904\u7406\u54cd\u5e94\u5185\u5bb9\n            # iter_lines() \u8fed\u4ee3\u54cd\u5e94\u5185\u5bb9\uff0c\u6309\u884c\u5206\u5272\uff0c\u5e76\u89e3\u7801\n            time2 = time.time()\n            for line in response.iter_lines():\n                # lines are bytes, convert to string\n                line = line.decode('utf-8')\n\n                # Server-Sent Events (SSE) messages start with 'data: '\n                # and the stream ends with 'data: [DONE]'\n                if line.startswith('data:'):\n                    # Extract the JSON string after 'data: '\n                    json_str = line[len('data:'):].strip()\n\n                    if json_str == '[DONE]':\n                        break # End of stream\n\n                    if json_str: # Ensure it's not an empty data line\n                        try:\n                            # Parse the JSON string into a dictionary\n                            chunk = json.loads(json_str)\n\n                            # Extract the content chunk (similar structure to OpenAI API)\n                            # Check if choices and delta exist before accessing content\n                            if chunk and 'choices' in chunk and len(chunk['choices']) &gt; 0:\n                                delta = chunk['choices'][0].get('delta')\n                                if delta and 'content' in delta:\n                                    content_chunk = delta['content']\n                                    # Print the chunk without a newline, immediately flushing the output\n                                    # print(content_chunk, end='', flush=True)\n                                    yield content_chunk\n\n                        except json.JSONDecodeError as e:\n                            print(f\"\\nError decoding JSON chunk: {e}\\nChunk: {json_str}\")\n                        except Exception as e:\n                            print(f\"\\nError processing chunk: {e}\\nChunk data: {chunk}\")\n\n\n            print(f\"\\n(Streaming finished) {time2-time1}\") # Add a newline after the stream is complete\n\n        else:\n            # Handle non-200 responses\n            print(f\"Error: Received status code {response.status_code}\")\n            print(\"Response body:\")\n            print(response.text) # Print the full error response if not streaming\n\n    except requests.exceptions.RequestException as e:\n        # TODO if \"439\" in ccc\n        print(f\"Request Error: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"api_document/#core","title":"core","text":"<p>\u6a21\u578b\u9002\u914d\u5668</p>"},{"location":"api_document/#src.llmada.core.ArkAdapter","title":"<code>ArkAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> Source code in <code>src/llmada/core.py</code> <pre><code>class ArkAdapter(ModelAdapter):\n    def __init__(self, api_key: str = None, api_base: str = None,):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n\n        self.client = Ark(api_key=api_key or os.getenv('ARK_API_KEY'))\n        self.model_pool = [\"doubao-1-5-pro-256k-250115\"]\n        self.model_name = self.model_pool[0]\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"api_document/#src.llmada.core.ArkAdapter.__init__","title":"<code>__init__(api_key=None, api_base=None)</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>None</code> Source code in <code>src/llmada/core.py</code> <pre><code>def __init__(self, api_key: str = None, api_base: str = None,):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n\n    self.client = Ark(api_key=api_key or os.getenv('ARK_API_KEY'))\n    self.model_pool = [\"doubao-1-5-pro-256k-250115\"]\n    self.model_name = self.model_pool[0]\n</code></pre>"},{"location":"api_document/#src.llmada.core.ArkAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"api_document/#src.llmada.core.ArkAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter","title":"<code>BianXieAdapter</code>","text":"<p>               Bases: <code>ModelModalAdapter</code></p> <p>BianXie\u683c\u5f0f\u7684\u9002\u914d\u5668</p> Source code in <code>src/llmada/core.py</code> <pre><code>class BianXieAdapter(ModelModalAdapter):\n    \"\"\"BianXie\u683c\u5f0f\u7684\u9002\u914d\u5668\n    \"\"\"\n    def __init__(self, api_key: str = None, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n        self.client = OpenAIClient(api_key=api_key or os.getenv('BIANXIE_API_KEY') , api_base=api_base)\n        self.model_pool = ['gemini-2.5-flash-preview-04-17-nothinking',\n                'gpt-3.5-turbo',\n                'gpt-4.1',\n                'gpt-4.1-2025-04-14',\n                'gpt-4.1-mini',\n                'gpt-4.1-mini-2025-04-14',\n                'gpt-4.1-nano',\n                'gpt-4.1-nano-2025-04-14',\n                'gpt-4o',\n                'gpt-4o-2024-11-20',\n                'gpt-4o-audio-preview',\n                'gpt-4o-audio-preview-2024-10-01',\n                'gpt-4o-audio-preview-2024-12-17',\n                'gpt-4o-all',\n                'gpt-4o-image',\n                'gpt-4o-image-vip',\n                'gpt-4o-mini',\n                'gpt-4o-mini-2024-07-18',\n                'gpt-4o-mini-audio-preview',\n                'gpt-4o-mini-audio-preview-2024-12-17',\n                'gpt-4o-mini-realtime-preview',\n                'gpt-4o-mini-realtime-preview-2024-12-17',\n                'gpt-4o-mini-search-preview',\n                'gpt-4o-mini-search-preview-2025-03-11',\n                'gpt-4o-realtime-preview',\n                'gpt-4o-realtime-preview-2024-10-01',\n                'gpt-4o-realtime-preview-2024-12-17',\n                'gpt-4o-search-preview-2025-03-11',\n                'gpt-4o-search-preview',\n                'claude-3-5-haiku-20241022',\n                'claude-3-5-haiku-latest',\n                'claude-3-5-sonnet-20240620',\n                'claude-3-5-sonnet-20241022',\n                'claude-3-5-sonnet-20241022-all',\n                'claude-3-5-sonnet-all',\n                'claude-3-5-sonnet-latest',\n                'claude-3-7-sonnet-20250219',\n                'claude-3-7-sonnet-20250219-thinking',\n                'claude-3-haiku-20240307',\n                'coder-claude3.5-sonnet',\n                'coder-claude3.7-sonnet',\n                'gemini-2.0-flash',\n                'gemini-2.0-flash-exp',\n                'gemini-2.0-flash-exp-image-generation',\n                'gemini-2.0-flash-thinking-exp',\n                'gemini-2.0-flash-thinking-exp-01-21',\n                'gemini-2.0-pro-exp-02-05',\n                'gemini-2.5-flash-preview-04-17',\n                'gemini-2.5-flash-preview-04-17-thinking',\n                'gemini-2.5-pro-exp-03-25',\n                'gemini-2.5-pro-preview-03-25',\n                'grok-3',\n                'grok-3-beta',\n                'grok-3-deepsearch',\n                'grok-3-mini-beta',\n                'grok-3-fast-beta',\n                'grok-3-mini-fast-beta',\n                'grok-3-reasoner',\n                'grok-beta',\n                'grok-vision-beta',\n                'o1-mini',\n                'o1-mini-2024-09-12',\n                'o3-mini',\n                'o3-mini-2025-01-31',\n                'o3-mini-all',\n                'o3-mini-high',\n                'o3-mini-low',\n                'o3-mini-medium',\n                'o4-mini',\n                'o4-mini-2025-04-16',\n                'o4-mini-high',\n                'o4-mini-medium',\n                'o4-mini-low',\n                'text-embedding-ada-002',\n                'text-embedding-3-small',\n                'text-embedding-3-large']\n        self.model_name = self.model_pool[0]\n        self.chat_history = []\n\n    def get_modal_model(self):\n        return ['gpt-4o']\n\n    def product_modal(self, prompt: RichPromptTemplate) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        assert self.model_name in self.get_modal_model()\n        messages = to_openai_message_dicts(prompt)\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        return self.client.request(data).get('choices')[0].get('message').get('content')\n\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        try:\n            return self.client.request(data).get('choices')[0].get('message').get('content')\n        except TypeError as e:\n            return e\n\n    def product_stream(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature,\n            \"stream\": True\n        }\n        result_stream = self.client.request_stream(data)\n\n        for i in result_stream:\n            yield i\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature\n        }\n        try:\n            return self.client.request(data).get('choices')[0].get('message').get('content')\n        except TypeError as e:\n            return e\n\n    def chat_stream(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n\n        data = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            'temperature': self.temperature,\n            \"stream\": True\n        }\n        result_stream = self.client.request_stream(data)\n\n        for i in result_stream:\n            yield i\n\n    def chat_stream_history(self, prompt: str, system:str = '') -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        if self.chat_history == [] and system != '':\n            self.chat_history.append({'role':'system','content':system})\n        self.chat_history.append({'role':'user','content':prompt})\n        data = {\n            \"model\": self.model_name,\n            \"messages\": self.chat_history,\n            'temperature': self.temperature,\n            \"stream\": True\n        }\n\n        result_stream = self.client.request_stream(data)\n\n        result_str = ''\n        for i in result_stream:\n            result_str += i\n            yield i\n        self.chat_history.append({'role':'assistant','content':result_str})\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.__init__","title":"<code>__init__(api_key=None, api_base='https://api.bianxie.ai/v1/chat/completions')</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>'https://api.bianxie.ai/v1/chat/completions'</code> Source code in <code>src/llmada/core.py</code> <pre><code>def __init__(self, api_key: str = None, api_base: str = \"https://api.bianxie.ai/v1/chat/completions\"):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n    self.client = OpenAIClient(api_key=api_key or os.getenv('BIANXIE_API_KEY') , api_base=api_base)\n    self.model_pool = ['gemini-2.5-flash-preview-04-17-nothinking',\n            'gpt-3.5-turbo',\n            'gpt-4.1',\n            'gpt-4.1-2025-04-14',\n            'gpt-4.1-mini',\n            'gpt-4.1-mini-2025-04-14',\n            'gpt-4.1-nano',\n            'gpt-4.1-nano-2025-04-14',\n            'gpt-4o',\n            'gpt-4o-2024-11-20',\n            'gpt-4o-audio-preview',\n            'gpt-4o-audio-preview-2024-10-01',\n            'gpt-4o-audio-preview-2024-12-17',\n            'gpt-4o-all',\n            'gpt-4o-image',\n            'gpt-4o-image-vip',\n            'gpt-4o-mini',\n            'gpt-4o-mini-2024-07-18',\n            'gpt-4o-mini-audio-preview',\n            'gpt-4o-mini-audio-preview-2024-12-17',\n            'gpt-4o-mini-realtime-preview',\n            'gpt-4o-mini-realtime-preview-2024-12-17',\n            'gpt-4o-mini-search-preview',\n            'gpt-4o-mini-search-preview-2025-03-11',\n            'gpt-4o-realtime-preview',\n            'gpt-4o-realtime-preview-2024-10-01',\n            'gpt-4o-realtime-preview-2024-12-17',\n            'gpt-4o-search-preview-2025-03-11',\n            'gpt-4o-search-preview',\n            'claude-3-5-haiku-20241022',\n            'claude-3-5-haiku-latest',\n            'claude-3-5-sonnet-20240620',\n            'claude-3-5-sonnet-20241022',\n            'claude-3-5-sonnet-20241022-all',\n            'claude-3-5-sonnet-all',\n            'claude-3-5-sonnet-latest',\n            'claude-3-7-sonnet-20250219',\n            'claude-3-7-sonnet-20250219-thinking',\n            'claude-3-haiku-20240307',\n            'coder-claude3.5-sonnet',\n            'coder-claude3.7-sonnet',\n            'gemini-2.0-flash',\n            'gemini-2.0-flash-exp',\n            'gemini-2.0-flash-exp-image-generation',\n            'gemini-2.0-flash-thinking-exp',\n            'gemini-2.0-flash-thinking-exp-01-21',\n            'gemini-2.0-pro-exp-02-05',\n            'gemini-2.5-flash-preview-04-17',\n            'gemini-2.5-flash-preview-04-17-thinking',\n            'gemini-2.5-pro-exp-03-25',\n            'gemini-2.5-pro-preview-03-25',\n            'grok-3',\n            'grok-3-beta',\n            'grok-3-deepsearch',\n            'grok-3-mini-beta',\n            'grok-3-fast-beta',\n            'grok-3-mini-fast-beta',\n            'grok-3-reasoner',\n            'grok-beta',\n            'grok-vision-beta',\n            'o1-mini',\n            'o1-mini-2024-09-12',\n            'o3-mini',\n            'o3-mini-2025-01-31',\n            'o3-mini-all',\n            'o3-mini-high',\n            'o3-mini-low',\n            'o3-mini-medium',\n            'o4-mini',\n            'o4-mini-2025-04-16',\n            'o4-mini-high',\n            'o4-mini-medium',\n            'o4-mini-low',\n            'text-embedding-ada-002',\n            'text-embedding-3-small',\n            'text-embedding-3-large']\n    self.model_name = self.model_pool[0]\n    self.chat_history = []\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    try:\n        return self.client.request(data).get('choices')[0].get('message').get('content')\n    except TypeError as e:\n        return e\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.chat_stream","title":"<code>chat_stream(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat_stream(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n\n    data = {\n        \"model\": self.model_name,\n        \"messages\": messages,\n        'temperature': self.temperature,\n        \"stream\": True\n    }\n    result_stream = self.client.request_stream(data)\n\n    for i in result_stream:\n        yield i\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.chat_stream_history","title":"<code>chat_stream_history(prompt, system='')</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat_stream_history(self, prompt: str, system:str = '') -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    if self.chat_history == [] and system != '':\n        self.chat_history.append({'role':'system','content':system})\n    self.chat_history.append({'role':'user','content':prompt})\n    data = {\n        \"model\": self.model_name,\n        \"messages\": self.chat_history,\n        'temperature': self.temperature,\n        \"stream\": True\n    }\n\n    result_stream = self.client.request_stream(data)\n\n    result_str = ''\n    for i in result_stream:\n        result_str += i\n        yield i\n    self.chat_history.append({'role':'assistant','content':result_str})\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    try:\n        return self.client.request(data).get('choices')[0].get('message').get('content')\n    except TypeError as e:\n        return e\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.product_modal","title":"<code>product_modal(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product_modal(self, prompt: RichPromptTemplate) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    assert self.model_name in self.get_modal_model()\n    messages = to_openai_message_dicts(prompt)\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature\n    }\n    return self.client.request(data).get('choices')[0].get('message').get('content')\n</code></pre>"},{"location":"api_document/#src.llmada.core.BianXieAdapter.product_stream","title":"<code>product_stream(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product_stream(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature,\n        \"stream\": True\n    }\n    result_stream = self.client.request_stream(data)\n\n    for i in result_stream:\n        yield i\n</code></pre>"},{"location":"api_document/#src.llmada.core.GoogleAdapter","title":"<code>GoogleAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> Source code in <code>src/llmada/core.py</code> <pre><code>class GoogleAdapter(ModelAdapter):\n    def __init__(self, api_key: str = None):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n        \"\"\"\n        super().__init__()\n\n        self.api_key = api_key or os.getenv('GOOGLE_API_KEY')\n        self.client = genai.Client(api_key=self.api_key)\n        self.model_pool = [\"gemini-2.5-flash-preview-04-17\"]\n        self.model_name = self.model_pool[0]\n        self.chat_session = None\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n        response = self.client.models.generate_content(\n            model=self.model_name,\n            config=types.GenerateContentConfig(\n                # system_instruction=\"You are a cat. Your name is Neko.\",\n                temperature=self.temperature),\n            contents=[prompt]\n        )\n        return response.text\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        # Create a new chat session if one doesn't exist\n        if self.chat_session is None:\n            self.chat_session = self.client.chats.create(model=self.model_name)\n\n        # Get the latest user message (usually the last one in the list)\n        latest_message = None\n        for msg in reversed(messages):\n            if msg.get('role') == 'user':\n                latest_message = msg.get('content', '')\n                break\n\n        if latest_message is None:\n            return \"No user message found\"\n\n        # Send the message to the chat session\n        response = self.chat_session.send_message(\n            latest_message,\n            config=types.GenerateContentConfig(\n                # system_instruction=\"You are a cat. Your name is Neko.\",\n                temperature=self.temperature),\n        )\n\n        return response.text\n</code></pre>"},{"location":"api_document/#src.llmada.core.GoogleAdapter.__init__","title":"<code>__init__(api_key=None)</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> <code>None</code> Source code in <code>src/llmada/core.py</code> <pre><code>def __init__(self, api_key: str = None):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n    \"\"\"\n    super().__init__()\n\n    self.api_key = api_key or os.getenv('GOOGLE_API_KEY')\n    self.client = genai.Client(api_key=self.api_key)\n    self.model_pool = [\"gemini-2.5-flash-preview-04-17\"]\n    self.model_name = self.model_pool[0]\n    self.chat_session = None\n</code></pre>"},{"location":"api_document/#src.llmada.core.GoogleAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    # Create a new chat session if one doesn't exist\n    if self.chat_session is None:\n        self.chat_session = self.client.chats.create(model=self.model_name)\n\n    # Get the latest user message (usually the last one in the list)\n    latest_message = None\n    for msg in reversed(messages):\n        if msg.get('role') == 'user':\n            latest_message = msg.get('content', '')\n            break\n\n    if latest_message is None:\n        return \"No user message found\"\n\n    # Send the message to the chat session\n    response = self.chat_session.send_message(\n        latest_message,\n        config=types.GenerateContentConfig(\n            # system_instruction=\"You are a cat. Your name is Neko.\",\n            temperature=self.temperature),\n    )\n\n    return response.text\n</code></pre>"},{"location":"api_document/#src.llmada.core.GoogleAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n    response = self.client.models.generate_content(\n        model=self.model_name,\n        config=types.GenerateContentConfig(\n            # system_instruction=\"You are a cat. Your name is Neko.\",\n            temperature=self.temperature),\n        contents=[prompt]\n    )\n    return response.text\n</code></pre>"},{"location":"api_document/#src.llmada.core.KimiAdapter","title":"<code>KimiAdapter</code>","text":"<p>               Bases: <code>ModelAdapter</code></p> <p>Kimi\u683c\u5f0f\u7684\u9002\u914d\u5668</p> Source code in <code>src/llmada/core.py</code> <pre><code>class KimiAdapter(ModelAdapter):\n    \"\"\"Kimi\u683c\u5f0f\u7684\u9002\u914d\u5668\n\n    \"\"\"\n    def __init__(self, api_key: str = None, api_base: str = \"https://api.moonshot.cn/v1\",):\n        \"\"\"\u521d\u59cb\u5316\n\n        Args:\n            api_key (str): API key for authentication.\n            api_base (str): Base URL for the API endpoint.\n        \"\"\"\n        super().__init__()\n\n        self.client = OpenAI(api_key=api_key or os.getenv('MOONSHOT_API_KEY') , base_url=api_base)\n        self.model_pool = [\"moonshot-v1-128k\",\"moonshot-v1-128k\",\"moonshot-v1-128k\"]\n        self.model_name = self.model_pool[0]\n\n    def product(self, prompt: str) -&gt; str:\n        \"\"\"Generate a response from the model based on a single prompt.\n\n        Args:\n            prompt (str): The input text prompt to generate a response for.\n\n        Returns:\n            str: The response generated by the model.\n        \"\"\"\n\n        data = {\n            'model': self.model_name,\n            'messages': [{'role': 'user', 'content': prompt}],\n            'temperature': self.temperature\n        }\n        return self.client.chat.completions.create(**data).choices[0].message.content\n\n    def chat(self, messages: list) -&gt; str:\n        \"\"\"Engage in a conversation with the model using a list of messages.\n\n        Args:\n            messages (list): A list of message dictionaries, each containing a role and content.\n\n        Returns:\n            str: The response generated by the model for the conversation.\n        \"\"\"\n        data = {\n            'model': self.model_name,\n            'messages': messages,\n            'temperature': self.temperature,\n            'stream': True #\n        }\n        return self.client.chat.completions.create(**data)\n</code></pre>"},{"location":"api_document/#src.llmada.core.KimiAdapter.__init__","title":"<code>__init__(api_key=None, api_base='https://api.moonshot.cn/v1')</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>'https://api.moonshot.cn/v1'</code> Source code in <code>src/llmada/core.py</code> <pre><code>def __init__(self, api_key: str = None, api_base: str = \"https://api.moonshot.cn/v1\",):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n    super().__init__()\n\n    self.client = OpenAI(api_key=api_key or os.getenv('MOONSHOT_API_KEY') , base_url=api_base)\n    self.model_pool = [\"moonshot-v1-128k\",\"moonshot-v1-128k\",\"moonshot-v1-128k\"]\n    self.model_name = self.model_pool[0]\n</code></pre>"},{"location":"api_document/#src.llmada.core.KimiAdapter.chat","title":"<code>chat(messages)</code>","text":"<p>Engage in a conversation with the model using a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>A list of message dictionaries, each containing a role and content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model for the conversation.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def chat(self, messages: list) -&gt; str:\n    \"\"\"Engage in a conversation with the model using a list of messages.\n\n    Args:\n        messages (list): A list of message dictionaries, each containing a role and content.\n\n    Returns:\n        str: The response generated by the model for the conversation.\n    \"\"\"\n    data = {\n        'model': self.model_name,\n        'messages': messages,\n        'temperature': self.temperature,\n        'stream': True #\n    }\n    return self.client.chat.completions.create(**data)\n</code></pre>"},{"location":"api_document/#src.llmada.core.KimiAdapter.product","title":"<code>product(prompt)</code>","text":"<p>Generate a response from the model based on a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input text prompt to generate a response for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The response generated by the model.</p> Source code in <code>src/llmada/core.py</code> <pre><code>def product(self, prompt: str) -&gt; str:\n    \"\"\"Generate a response from the model based on a single prompt.\n\n    Args:\n        prompt (str): The input text prompt to generate a response for.\n\n    Returns:\n        str: The response generated by the model.\n    \"\"\"\n\n    data = {\n        'model': self.model_name,\n        'messages': [{'role': 'user', 'content': prompt}],\n        'temperature': self.temperature\n    }\n    return self.client.chat.completions.create(**data).choices[0].message.content\n</code></pre>"},{"location":"api_document/#src.llmada.core.set_llama_index","title":"<code>set_llama_index(api_key=None, api_base='https://api.bianxieai.com/v1', model='gpt-4o', temperature=0.1, llm_config={}, embed_config={})</code>","text":"<p>\u521d\u59cb\u5316</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authentication.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>'https://api.bianxieai.com/v1'</code> Source code in <code>src/llmada/core.py</code> <pre><code>def set_llama_index(api_key: str = None, api_base: str = \"https://api.bianxieai.com/v1\",model:str=\"gpt-4o\",temperature: float =0.1,\n                    llm_config:dict = {},embed_config:dict = {}):\n    \"\"\"\u521d\u59cb\u5316\n\n    Args:\n        api_key (str): API key for authentication.\n        api_base (str): Base URL for the API endpoint.\n    \"\"\"\n\n    from llama_index.llms.openai import OpenAI\n    from llama_index.core import Settings\n    from llama_index.embeddings.openai import OpenAIEmbedding\n\n    api_key=api_key or os.getenv('BIANXIE_API_KEY')\n\n    client = OpenAI(\n        model=model,\n        api_base=api_base,\n        api_key=api_key,\n        temperature=temperature,\n        **llm_config\n    )\n    embedding = OpenAIEmbedding(api_base=api_base,api_key=api_key,**embed_config)\n    Settings.embed_model = embedding\n    Settings.llm = client\n</code></pre>"},{"location":"use_case/","title":"Use Case","text":"<pre><code>%cd ~/GitHub/llmada/src\n</code></pre> <pre><code>/Users/zhaoxuefeng/GitHub/llmada/src\n</code></pre>"},{"location":"use_case/#bianxie","title":"bianxie","text":"<pre><code>from llmada import BianXieAdapter\n</code></pre> <pre><code>bx = BianXieAdapter()\n</code></pre> <pre><code>bx.set_model('gpt-3.5-turbo')\n</code></pre> <pre><code>bx.product('hello')\n</code></pre> <pre><code>2.2560229301452637\n\n\n\n\n\n'Hello! How can I assist you today?'\n</code></pre> <pre><code>for i in bx.product_stream('hello'):\n    print(i, end=\"\")\n</code></pre> <pre><code>Received streaming response:\nHello! How can I assist you today?\n(Streaming finished) 1.8848471641540527\n</code></pre> <pre><code>bx.chat([{'role':'user','content':'\u4f60\u597d'}])\n</code></pre> <pre><code>1.8273937702178955\n\n\n\n\n\n'\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\u5417\uff1f'\n</code></pre> <pre><code>for i in bx.chat_stream_history('hello'):\n    print(i, end=\"\")\n</code></pre> <pre><code>Received streaming response:\nHello! How can I assist you today?\n(Streaming finished) 2.101140022277832\n</code></pre> <pre><code>for i in bx.chat_stream([{'role':'user','content':'\u4f60\u597d'}]):\n    print(i, end=\"\")\n</code></pre> <pre><code>Received streaming response:\n\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\u5417\uff1f\n(Streaming finished) 1.993128776550293\n</code></pre> <pre><code>bx.get_model()\n</code></pre> <pre><code>['gemini-2.5-flash-preview-04-17-nothinking',\n 'gpt-3.5-turbo',\n 'gpt-4.1',\n 'gpt-4.1-2025-04-14',\n 'gpt-4.1-mini',\n 'gpt-4.1-mini-2025-04-14',\n 'gpt-4.1-nano',\n 'gpt-4.1-nano-2025-04-14',\n 'gpt-4o',\n 'gpt-4o-2024-11-20',\n 'gpt-4o-audio-preview',\n 'gpt-4o-audio-preview-2024-10-01',\n 'gpt-4o-audio-preview-2024-12-17',\n 'gpt-4o-all',\n 'gpt-4o-image',\n 'gpt-4o-image-vip',\n 'gpt-4o-mini',\n 'gpt-4o-mini-2024-07-18',\n 'gpt-4o-mini-audio-preview',\n 'gpt-4o-mini-audio-preview-2024-12-17',\n 'gpt-4o-mini-realtime-preview',\n 'gpt-4o-mini-realtime-preview-2024-12-17',\n 'gpt-4o-mini-search-preview',\n 'gpt-4o-mini-search-preview-2025-03-11',\n 'gpt-4o-realtime-preview',\n 'gpt-4o-realtime-preview-2024-10-01',\n 'gpt-4o-realtime-preview-2024-12-17',\n 'gpt-4o-search-preview-2025-03-11',\n 'gpt-4o-search-preview',\n 'claude-3-5-haiku-20241022',\n 'claude-3-5-haiku-latest',\n 'claude-3-5-sonnet-20240620',\n 'claude-3-5-sonnet-20241022',\n 'claude-3-5-sonnet-20241022-all',\n 'claude-3-5-sonnet-all',\n 'claude-3-5-sonnet-latest',\n 'claude-3-7-sonnet-20250219',\n 'claude-3-7-sonnet-20250219-thinking',\n 'claude-3-haiku-20240307',\n 'coder-claude3.5-sonnet',\n 'coder-claude3.7-sonnet',\n 'gemini-2.0-flash',\n 'gemini-2.0-flash-exp',\n 'gemini-2.0-flash-exp-image-generation',\n 'gemini-2.0-flash-thinking-exp',\n 'gemini-2.0-flash-thinking-exp-01-21',\n 'gemini-2.0-pro-exp-02-05',\n 'gemini-2.5-flash-preview-04-17',\n 'gemini-2.5-flash-preview-04-17-thinking',\n 'gemini-2.5-pro-exp-03-25',\n 'gemini-2.5-pro-preview-03-25',\n 'grok-3',\n 'grok-3-beta',\n 'grok-3-deepsearch',\n 'grok-3-mini-beta',\n 'grok-3-fast-beta',\n 'grok-3-mini-fast-beta',\n 'grok-3-reasoner',\n 'grok-beta',\n 'grok-vision-beta',\n 'o1-mini',\n 'o1-mini-2024-09-12',\n 'o3-mini',\n 'o3-mini-2025-01-31',\n 'o3-mini-all',\n 'o3-mini-high',\n 'o3-mini-low',\n 'o3-mini-medium',\n 'o4-mini',\n 'o4-mini-2025-04-16',\n 'o4-mini-high',\n 'o4-mini-medium',\n 'o4-mini-low',\n 'text-embedding-ada-002',\n 'text-embedding-3-small',\n 'text-embedding-3-large']\n</code></pre> <pre><code>bx.get_modal_model()\n</code></pre> <pre><code>['gpt-4o']\n</code></pre> <pre><code># TODO\nproduct_modal\nchat_modal\nproduct_stream_modal\nchat_stream_modal\nchat_stream_history_modal\n</code></pre> <pre><code>from llmada import set_llama_index\n</code></pre> <pre><code>set_llama_index(llm_config = {\"api_key\":\"123423\"})\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\nCell In[17], line 1\n----&gt; 1 set_llama_index(llm_config = {\"api_key\":\"123423\"})\n\n\nFile ~/GitHub/llmada/src/llmada/core.py:448, in set_llama_index(api_key, api_base, model, temperature, llm_config, embed_config)\n    444 from llama_index.embeddings.openai import OpenAIEmbedding\n    446 api_key=api_key or os.getenv('BIANXIE_API_KEY')\n--&gt; 448 client = OpenAI(\n    449     model=model,\n    450     api_base=api_base,\n    451     api_key=api_key,\n    452     temperature=temperature,\n    453     **llm_config\n    454 )\n    455 embedding = OpenAIEmbedding(api_base=api_base,api_key=api_key,**embed_config)\n    456 Settings.embed_model = embedding\n\n\nTypeError: llama_index.llms.openai.base.OpenAI() got multiple values for keyword argument 'api_key'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"use_case/#bianxie_1","title":"bianxie","text":"<pre><code>from llmada.core import GoogleAdapter\n</code></pre> <pre><code>model = GoogleAdapter()\n</code></pre> <pre><code>model.model_name\n</code></pre> <pre><code>'gemini-2.5-flash-preview-04-17'\n</code></pre> <pre><code>model.product('\u4f60\u597d')\n</code></pre> <pre><code>'\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u4f60\u7684\u5417\uff1f'\n</code></pre> <pre><code>model.chat([{'role':'user','content':'\u4f60\u597d'}])\n</code></pre> <pre><code>'\u4f60\u597d\uff01\u6709\u4ec0\u4e48\u6211\u53ef\u4ee5\u5e2e\u60a8\u7684\u5417\uff1f'\n</code></pre>"}]}